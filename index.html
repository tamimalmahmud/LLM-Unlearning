<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <meta name="description" content="LLM-Unlearning: A framework for unlearning in Large Language Models for privacy-preserving AI.">
    <meta name="keywords" content="LLM unlearning, Exact Unlearning, Efficient Unlearning, Approximate Unlearning, Trustworthy AI, machine unlearning, privacy-preserving AI, open-source">
    <title>LLM-Unlearning: Privacy-Preserving Unlearning for Trustworthy AI</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>LLM-Unlearning: Privacy-Preserving Unlearning for Trustworthy AI</h1>
        <p>LLM-Unlearning is an open-source project focused on enabling unlearning techniques in Large Language Models (LLMs). With the rise of AI applications and increasing concerns about data privacy, this project introduces exact and approximate unlearning methods designed to make AI models privacy-preserving, trustworthy, and ethical.</p>
    </header>
    <section>
        <h2>Project Overview</h2>
        <p>By facilitating models to "forget" unwanted or sensitive data, LLM-Unlearning helps ensure compliance with data privacy regulations (i.e., GDPR, CCPA) and fosters ethical AI development. It is a crucial step toward creating AI models that are both transparent and fair, while maintaining high performance.</p>
    </section>
    <section>
        <h2>Key Features</h2>
        <ul>
            <li><strong>Exact and Approximate Unlearning:</strong> Methods for forgetting specific data efficiently, while preserving model performance.</li>
            <li><strong>Privacy-Preserving AI:</strong> Secure mechanisms to allow models to forget sensitive data and protect user privacy.</li>
            <li><strong>Trustworthy AI:</strong> Promotes building ethical models that offer transparency and fairness in data processing.</li>
        </ul>
    </section>
    <section>
        <h2>Project 1: DP2Unlearning [<a href="https://github.com/tamimalmahmud/LLM-Unlearning/tree/main/DP2Unlearning">Github</a>]</h2>
        <h3>Paper: <a href="http://ssrn.com/abstract=5217160">DP2Unlearning: An Efficient and Guaranteed Unlearning Framework for LLMs</a></h3>
        <p>The DP2Unlearning project focuses on advanced techniques for unlearning within LLMs, offering an efficient and guaranteed framework for data privacy preservation. Navigate to the DP2Unlearning project directory to explore and simulate the results. You can also develop and adapt the methods to your own ideas and research needs.</p>
    </section>
    <section>
        <h2>How to Get Started</h2>
        <ol>
            <li>Navigate to the DP2Unlearning directory.</li>
            <li>Follow the setup instructions in the repository to replicate the results from the paper.</li>
            <li>Experiment with unlearning methods, apply them to your use cases, and share your insights!</li>
        </ol>
    </section>

    <section>
    </section>
    
    <footer>
        <p>Created by <a href="https://github.com/tamimalmahmud">Tamim Al Mahmud</a></p>
        <p>&copy; 2025 All rights reserved.</p>
    </footer>
</body>
</html>
